AWSTemplateFormatVersion: 2010-09-09
Transform: AWS::Serverless-2016-10-31

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label: Build Info
        Parameters:
          - CodePipeline
          - BuildRegion
          - SourceVersion
          - Build
      - Label: Code Location
        Parameters:
          - CodeBucket
          - CodePrefix
      - Label: EC2
        Parameters:
          - KeyPair
      - Label: Deployment Configuration
        Parameters:
          - Stage
          - DeploymentPreference
      - Label: Task Runner
        Parameters:
          - AccessKeyID
          - SecretAccessKey
      - Label: VPC
        Parameters:
          - Subnet
          - SecurityGroup
      - Label: Data Infrastructure
        Parameters:
          - EnableEC2Instance
          - EnableEMRCluster
      - Label: Redshift
        Parameters:
          - RedshiftClusterId
          - RedshiftDatabase
          - RedshiftUsername
          - RedshiftPassword
    ParameterLabels:
      CodePipeline:
        default: The CodePipeline pipeline that build this app
      BuildRegion:
        default: The region the app was built in
      SourceVersion:
        default: The source version built
      Build:
        default: The unique build ID
      CodeBucket:
        default: Bucket containing scripts
      CodePrefix:
        default: Prefix for scripts
      KeyPair:
        default: EC2 KeyPair
      Stage:
        default: Stage to deploy
      DeploymentPreference:
        default: CodeDeploy deployment preference
      AccessKeyID:
        default: AWS Access Key ID
      SecretAccessKey:
        default: AWS Secret Access Key
      EnableEC2Instance:
        default: The EC2 instance is enabled when non-nil
      EnableEMRCluster:
        default: The EMR instance is enabled when non-nil
      RedshiftClusterId:
        default: The Redshift cluster ID when non-nil
      RedshiftDatabase:
        default: The default Redshift database
      RedshiftUsername:
        default: The Redshift username
      RedshiftPassword:
        default: The Redshift password

Parameters:
  CodePipeline:
    Type: String
    Description: The CodePipeline pipeline that build this app
  BuildRegion:
    Type: String
    Description: The region the app was built in
  SourceVersion:
    Type: String
    Description: The source version built
  Build:
    Type: String
    Description: The unique build ID
  CodeBucket:
    Type: String
    Description: The S3 bucket user code is stored in.
  CodePrefix:
    Type: String
    Description: The prefix into the S3 bucket code is stored in.
  KeyPair:
    Type: String
    Description: The EC2 KeyPair to use
  Stage:
    Type: String
    Description: The name for a project pipeline stage, such as Staging or Prod, for which resources are provisioned and deployed.
    Default: ''
  DeploymentPreference:
    Type: String
    Description: The CodeDeploy deployment preference for safe Lambda deploys.
    Default: Canary10Percent5Minutes
    AllowedValues:
      - Canary10Percent5Minutes
      - Canary10Percent10Minutes
      - Canary10Percent15Minutes
      - Canary10Percent30Minutes
      - Linear10PercentEvery1Minutes
      - Linear10PercentEvery2Minutes
      - Linear10PercentEvery3Minutes
      - Linear10PercentEvery10Minutes
      - AllAtOnce
  Subnet:
    Type: String
    Description: The subnet external resources are running in
    Default: ''
  SecurityGroup:
    Type: String
    Description: A security group to access the external resources
    Default: ''
  AccessKeyID:
    Type: String
    Description: The AWS Access Key ID to use for the Data Pipeline Task Runner
    Default: ''
  SecretAccessKey:
    Type: String
    Description: The AWS Secret Access Key to use for the Data Pipeline Task Runner
    Default: ''
    NoEcho: true
  BucketName:
    Type: String
    Description: The name of the data lake bucket
  BucketARN:
    Type: String
    Description: The ARN of the data lake bucket
  EnableEC2Instance:
    Type: String
    Description: If non-empty, the parent created an EC2 instance for use by Data Pipeline.
    Default: ''
  EnableEMRCluster:
    Type: String
    Description: If non-empty, the parent created an EMR cluster.
    Default: ''
  RedshiftClusterId:
    Type: String
    Description: The ID of the Redshift cluster to use.
    Default: ''
  RedshiftDatabase:
    Type: String
    Description: The name of the default Redshift database
    Default: dataless
  RedshiftUsername:
    Type: String
    Description: The username of the default Redshift user
    Default: dataless
  RedshiftPassword:
    Type: String
    Description: The password of the default Redshift user
    Default: dataless
    NoEcho: true

Conditions:
  EC2InstanceEnabled: !And
    - !Not [!Equals [!Ref EnableEC2Instance, ""]]
    - !Not [!Equals [!Ref AccessKeyID, ""]]
    - !Not [!Equals [!Ref SecretAccessKey, ""]]
  EMREnabled: !And
    - !Not [!Equals [!Ref EnableEMRCluster, ""]]
    - !Not [!Equals [!Ref AccessKeyID, ""]]
    - !Not [!Equals [!Ref SecretAccessKey, ""]]
  RedshiftEnabled: !And
    - !Not [!Equals [!Ref RedshiftClusterId, ""]]
    - !Not [!Equals [!Ref RedshiftDatabase, ""]]
    - !Not [!Equals [!Ref RedshiftUsername, ""]]
    - !Not [!Equals [!Ref RedshiftPassword, ""]]

Outputs:
  API:
    Value: !Ref API
  Stage:
    Value: !Ref API.Stage
  BaseURL:
    Value: !Sub "https://${API}.execute-api.${AWS::Region}.amazonaws.com/${Stage}"

Globals:
  Function:
    Runtime: go1.x
    Tracing: Active
    AutoPublishAlias: Live
    DeploymentPreference:
      Enabled: true
      Type: !Ref DeploymentPreference
    Environment:
      Variables:
        IMPRESSIONS_DELIVERY_STREAM_NAME: !Ref ImpressionsDeliveryStream
        CLICKS_DELIVERY_STREAM_NAME: !Ref ClicksDeliveryStream
        AD_TRAFFIC_TABLE_NAME: !Ref AdTrafficTable
  Api:
    TracingEnabled: true
    EndpointConfiguration: EDGE
  SimpleTable:
    SSESpecification:
      SSEEnabled: true

Resources:

  ####################
  # Delivery Streams #
  ####################

  DeliveryStreamRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - !Sub "firehose.${AWS::URLSuffix}"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Policies:
        - PolicyName: Execution
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - s3:PutObject*
                Effect: Allow
                Resource:
                  - !Sub "${BucketARN}/data/raw/*"

  ImpressionsDeliveryStream:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      DeliveryStreamType: DirectPut
      S3DestinationConfiguration:
        BucketARN: !Ref BucketARN
        Prefix: data/raw/ad/impressions/
        BufferingHints:
          IntervalInSeconds: 900
          SizeInMBs: 128
        CompressionFormat: GZIP
        RoleARN: !GetAtt DeliveryStreamRole.Arn

  ImpressionsDeliveryStreamRecordsPerSecondAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      Statistic: Average
      Namespace: "AWS/Firehose"
      MetricName: IncomingRecords
      Dimensions:
        - Name: DeliveryStreamName
          Value: !Ref ImpressionsDeliveryStream
      ComparisonOperator: GreaterThanThreshold
      Threshold: 4000
      DatapointsToAlarm: 3
      EvaluationPeriods: 5
      Period: 60

  ImpressionsDeliveryStreamTransactionsPerSecondAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      Statistic: Average
      Namespace: "AWS/Firehose"
      MetricName: PutRecordBatch.Requests
      Dimensions:
        - Name: DeliveryStreamName
          Value: !Ref ImpressionsDeliveryStream
      ComparisonOperator: GreaterThanThreshold
      Threshold: 1600
      DatapointsToAlarm: 3
      EvaluationPeriods: 5
      Period: 60

  ImpressionsDeliveryStreamDataThroughputAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      Statistic: Average
      Namespace: "AWS/Firehose"
      MetricName: IncomingBytes
      Dimensions:
        - Name: DeliveryStreamName
          Value: !Ref ImpressionsDeliveryStream
      ComparisonOperator: GreaterThanThreshold
      Threshold: 4194304
      DatapointsToAlarm: 3
      EvaluationPeriods: 5
      Period: 60

  ClicksDeliveryStream:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      DeliveryStreamType: DirectPut
      S3DestinationConfiguration:
        BucketARN: !Ref BucketARN
        Prefix: data/raw/ad/clicks/
        BufferingHints:
          IntervalInSeconds: 900
          SizeInMBs: 128
        CompressionFormat: GZIP
        RoleARN: !GetAtt DeliveryStreamRole.Arn

  ClicksDeliveryStreamRecordsPerSecondAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      Statistic: Average
      Namespace: "AWS/Firehose"
      MetricName: IncomingRecords
      Dimensions:
        - Name: DeliveryStreamName
          Value: !Ref ClicksDeliveryStream
      ComparisonOperator: GreaterThanThreshold
      Threshold: 4000
      DatapointsToAlarm: 3
      EvaluationPeriods: 5
      Period: 60

  ClicksDeliveryStreamTransactionsPerSecondAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      Statistic: Average
      Namespace: "AWS/Firehose"
      MetricName: PutRecordBatch.Requests
      Dimensions:
        - Name: DeliveryStreamName
          Value: !Ref ClicksDeliveryStream
      ComparisonOperator: GreaterThanThreshold
      Threshold: 1600
      DatapointsToAlarm: 3
      EvaluationPeriods: 5
      Period: 60

  ClicksDeliveryStreamDataThroughputAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      Statistic: Average
      Namespace: "AWS/Firehose"
      MetricName: IncomingBytes
      Dimensions:
        - Name: DeliveryStreamName
          Value: !Ref ClicksDeliveryStream
      ComparisonOperator: GreaterThanThreshold
      Threshold: 4194304
      DatapointsToAlarm: 3
      EvaluationPeriods: 5
      Period: 60

  #############
  # Real Time #
  #############

  RealTimeAdvertisingDashboard:
    Type: AWS::CloudWatch::Dashboard
    Properties:
      DashboardBody: !Sub |
        {
          "start": "-PT1H",
          "periodOverride": "inherit",
          "widgets": [
            {
              "type": "text",
              "width": 24,
              "height": 1,
              "properties": {
                "markdown": "# Clickthrough Rate"
              }
            },
            {
              "type": "metric",
              "width": 8,
              "height": 6,
              "properties": {
                "metrics": [
                  ["Warehouse/Advertising", "Impressions"]
                ],
                "period": 60,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "title": "Impressions"
              }
            },
            {
              "type": "metric",
              "width": 8,
              "height": 6,
              "properties": {
                "metrics": [
                  ["Warehouse/Advertising", "Clicks"]
                ],
                "period": 60,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "title": "Clicks"
              }
            },
            {
              "type": "metric",
              "width": 8,
              "height": 6,
              "properties": {
                "metrics": [
                  ["Warehouse/Advertising", "ClickthroughRate"]
                ],
                "period": 60,
                "stat": "Average",
                "region": "${AWS::Region}",
                "title": "Clickthrough Rate"
              }
            }
          ]
        }

  RealTimeAdvertisingLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Action: sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - !Sub "lambda.${AWS::URLSuffix}"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/AWSXrayWriteOnlyAccess
      Policies:
        - PolicyName: Execution
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - cloudwatch:PutMetricData
                Effect: Allow
                Resource: "*"

  RealTimeAdvertisingLambda:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: lambdas.zip
      Handler: bin/publish-to-cloudwatch
      Role: !GetAtt RealTimeAdvertisingLambdaRole.Arn

  RealTimeAdvertisingApplicationRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - !Sub "kinesisanalytics.${AWS::URLSuffix}"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonKinesisAnalyticsFullAccess
      Policies:
        - PolicyName: Execution
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - lambda:InvokeFunction
                  - lambda:GetFunctionConfiguration
                Effect: Allow
                Resource:
                  - !GetAtt RealTimeAdvertisingLambda.Arn
              - Action:
                  - firehose:DescribeDeliveryStream
                  - firehose:Get*
                Effect: Allow
                Resource:
                  - !GetAtt ImpressionsDeliveryStream.Arn
                  - !GetAtt ClicksDeliveryStream.Arn

  RealTimeImpressionsApplication:
    Type: AWS::KinesisAnalytics::Application
    Properties:
      ApplicationCode: |
        CREATE OR REPLACE STREAM "metrics" (
          "namespace" VARCHAR(255),
          "name"      VARCHAR(255),
          "at"        TIMESTAMP,
          "value"     DOUBLE
        );

        CREATE OR REPLACE PUMP "counter" AS
        INSERT INTO "metrics"
        SELECT STREAM
          'Warehouse/Advertising'                                                                             AS "namespace",
          'Impressions'                                                                                       AS "name",
          CHAR_TO_TIMESTAMP('yyyy-MM-dd hh:mm', TIMESTAMP_TO_CHAR('yyyy-MM-dd hh:mm', "impressions".ROWTIME)) AS "at",
          CAST(COUNT(1) AS DOUBLE)                                                                            AS "value"
        FROM
          "impressions_001" AS "impressions"
        GROUP BY
          CHAR_TO_TIMESTAMP('yyyy-MM-dd hh:mm', TIMESTAMP_TO_CHAR('yyyy-MM-dd hh:mm', "impressions".ROWTIME)),
          STEP("impressions".ROWTIME BY INTERVAL '60' SECOND)
        ;
      Inputs:
        - NamePrefix: impressions
          KinesisFirehoseInput:
            ResourceARN: !GetAtt ImpressionsDeliveryStream.Arn
            RoleARN: !GetAtt RealTimeAdvertisingApplicationRole.Arn
          InputSchema:
            RecordEncoding: UTF-8
            RecordFormat:
              RecordFormatType: JSON
              MappingParameters:
                JSONMappingParameters:
                  RecordRowPath: "$"
            RecordColumns:
              - Name: ad_id
                SqlType: VARCHAR(64)
                Mapping: "$.ad_id"
              - Name: impression_id
                SqlType: VARCHAR(64)
                Mapping: "$.impression_id"
              - Name: occurred_at
                SqlType: TIMESTAMP
                Mapping: "$.occurred_at"

  RealTimeImpressionsOutputs:
    Type: AWS::KinesisAnalytics::ApplicationOutput
    Properties:
      ApplicationName: !Ref RealTimeImpressionsApplication
      Output:
        Name: metrics
        DestinationSchema:
          RecordFormatType: JSON
        LambdaOutput:
          ResourceARN: !GetAtt RealTimeAdvertisingLambda.Arn
          RoleARN: !GetAtt RealTimeAdvertisingApplicationRole.Arn

  RealTimeClicksApplication:
    Type: AWS::KinesisAnalytics::Application
    Properties:
      ApplicationCode: |
        CREATE OR REPLACE STREAM "metrics" (
          "namespace" VARCHAR(255),
          "name"      VARCHAR(255),
          "at"        TIMESTAMP,
          "value"     DOUBLE
        );

        CREATE OR REPLACE PUMP "counter" AS
        INSERT INTO "metrics"
        SELECT STREAM
          'Warehouse/Advertising'                                                                        AS "namespace",
          'Clicks'                                                                                       AS "name",
          CHAR_TO_TIMESTAMP('yyyy-MM-dd hh:mm', TIMESTAMP_TO_CHAR('yyyy-MM-dd hh:mm', "clicks".ROWTIME)) AS "at",
          CAST(COUNT(1) AS DOUBLE)                                                                       AS "value"
        FROM
          "clicks_001" AS "clicks"
        GROUP BY
          CHAR_TO_TIMESTAMP('yyyy-MM-dd hh:mm', TIMESTAMP_TO_CHAR('yyyy-MM-dd hh:mm', "clicks".ROWTIME)),
          STEP("clicks".ROWTIME BY INTERVAL '60' SECOND)
        ;
      Inputs:
        - NamePrefix: clicks
          KinesisFirehoseInput:
            ResourceARN: !GetAtt ClicksDeliveryStream.Arn
            RoleARN: !GetAtt RealTimeAdvertisingApplicationRole.Arn
          InputSchema:
            RecordEncoding: UTF-8
            RecordFormat:
              RecordFormatType: JSON
              MappingParameters:
                JSONMappingParameters:
                  RecordRowPath: "$"
            RecordColumns:
              - Name: ad_id
                SqlType: VARCHAR(64)
                Mapping: "$.ad_id"
              - Name: impression_id
                SqlType: VARCHAR(64)
                Mapping: "$.impression_id"
              - Name: click_id
                SqlType: VARCHAR(64)
                Mapping: "$.click_id"
              - Name: occurred_at
                SqlType: TIMESTAMP
                Mapping: "$.occurred_at"

  RealTimeClicksOutputs:
    Type: AWS::KinesisAnalytics::ApplicationOutput
    Properties:
      ApplicationName: !Ref RealTimeClicksApplication
      Output:
        Name: metrics
        DestinationSchema:
          RecordFormatType: JSON
        LambdaOutput:
          ResourceARN: !GetAtt RealTimeAdvertisingLambda.Arn
          RoleARN: !GetAtt RealTimeAdvertisingApplicationRole.Arn

  ####################
  # ETL To Data Lake #
  ####################

  Database:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: advertising

  CrawlerRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - !Sub "glue.${AWS::URLSuffix}"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Policies:
        - PolicyName: Execution
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - s3:GetObject*
                  - s3:PutObject*
                Effect: Allow
                Resource:
                  - !Sub "${BucketARN}/data/raw/*"
              - Action:
                  - s3:ListBucket*
                Effect: Allow
                Resource:
                  - !Ref BucketARN

  RawCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: raw_crawler
      Description: Crawls the raw data
      Role: !GetAtt CrawlerRole.Arn
      DatabaseName: !Ref Database
      Targets: # CloudFormation does not seem to support targeting an existing table.  Console does, at least.
        S3Targets:
          - Path: !Sub "s3://${BucketName}/data/raw/ad/impressions/"
          - Path: !Sub "s3://${BucketName}/data/raw/ad/clicks/"
      TablePrefix: raw_
      SchemaChangePolicy:
        UpdateBehavior: UPDATE_IN_DATABASE
        DeleteBehavior: LOG
      # Schedule:
      #   ScheduleExpression: cron(20 * * * ? *)
      Configuration: |
        {
          "Version": 1.0,
          "Grouping": {
            "TableGroupingPolicy": "CombineCompatibleSchemas"
          },
          "CrawlerOutput": {
            "Partitions": {
              "AddOrUpdateBehavior": "InheritFromTable"
            },
            "Tables": {
              "AddOrUpdateBehavior": "MergeNewColumns"
            }
          }
        }

  ImpressionsTable:
    Type: AWS::Glue::Table
    DependsOn:
      - Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref Database
      TableInput:
          Name: impressions
          Description: Ad Impressions
          TableType: EXTERNAL_TABLE
          Parameters:
            classification: orc
          PartitionKeys:
            - Name: year
              Type: int
            - Name: month
              Type: int
          StorageDescriptor:
            Location: !Sub "s3://${BucketName}/data/lake/ad/impressions/"
            StoredAsSubDirectories: true
            InputFormat: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            SerdeInfo:
              SerializationLibrary: org.apache.hadoop.hive.ql.io.orc.OrcSerde
            OutputFormat: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            Columns:
              - Name: impression_id
                Type: string
              - Name: ad_id
                Type: string
              - Name: occurred_at
                Type: timestamp

  ClicksTable:
    Type: AWS::Glue::Table
    DependsOn:
      - Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref Database
      TableInput:
          Name: clicks
          Description: Ad Clicks
          TableType: EXTERNAL_TABLE
          Parameters:
            classification: orc
          PartitionKeys:
            - Name: year
              Type: int
            - Name: month
              Type: int
          StorageDescriptor:
            Location: !Sub "s3://${BucketName}/data/lake/ad/clicks/"
            StoredAsSubDirectories: true
            InputFormat: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            SerdeInfo:
              SerializationLibrary: org.apache.hadoop.hive.ql.io.orc.OrcSerde
            OutputFormat: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            Columns:
              - Name: impression_id
                Type: string
              - Name: click_id
                Type: string
              - Name: ad_id
                Type: string
              - Name: occurred_at
                Type: timestamp

  ETLJobRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Action:
            - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - !Sub "glue.${AWS::URLSuffix}"
      Policies:
        - PolicyName: "root"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action: "*"
                Effect: Allow
                Resource: "*"

  ImpressionsPythonETLJob:
    Type: AWS::Glue::Job
    Properties:
      Role: !GetAtt ETLJobRole.Arn
      Command:
        Name: glueetl
        ScriptLocation: !Sub "s3://${CodeBucket}/${CodePrefix}/scripts/glue/impressions_etl.py"
      DefaultArguments:
        "--enable-glue-datacatalog": ""
        "--job-bookmark-option": "job-bookmark-enable"
        "--database_name": !Ref Database
        "--raw_table_name": raw_impressions # Hard-coded because CloudFormation-based Crawlers can't crawl an existing table
        "--table_name": !Ref ImpressionsTable
      ExecutionProperty:
        MaxConcurrentRuns: 1
      MaxRetries: 0

  ImpressionsScalaETLJob:
    Type: AWS::Glue::Job
    Properties:
      Role: !GetAtt ETLJobRole.Arn
      Command:
        Name: glueetl
        ScriptLocation: !Sub "s3://${CodeBucket}/${CodePrefix}/scripts/glue/impressions_etl.scala"
      DefaultArguments:
        "--job-language": scala
        "--class": GlueApp
        "--enable-glue-datacatalog": ""
        "--job-bookmark-option": "job-bookmark-enable"
        "--database_name": !Ref Database
        "--raw_table_name": raw_impressions # Hard-coded because CloudFormation-based Crawlers can't crawl an existing table
        "--table_name": !Ref ImpressionsTable
      ExecutionProperty:
        MaxConcurrentRuns: 1
      MaxRetries: 0

  ClicksPythonETLJob:
    Type: AWS::Glue::Job
    Properties:
      Role: !GetAtt ETLJobRole.Arn
      Command:
        Name: glueetl
        ScriptLocation: !Sub "s3://${CodeBucket}/${CodePrefix}/scripts/glue/clicks_etl.py"
      DefaultArguments:
        "--enable-glue-datacatalog": ""
        "--job-bookmark-option": "job-bookmark-enable"
        "--database_name": !Ref Database
        "--raw_table_name": raw_clicks # Hard-coded because CloudFormation-based Crawlers can't crawl an existing table
        "--table_name": !Ref ClicksTable
      ExecutionProperty:
        MaxConcurrentRuns: 1
      MaxRetries: 0

  ClicksScalaETLJob:
    Type: AWS::Glue::Job
    Properties:
      Role: !GetAtt ETLJobRole.Arn
      Command:
        Name: glueetl
        ScriptLocation: !Sub "s3://${CodeBucket}/${CodePrefix}/scripts/glue/clicks_etl.scala"
      DefaultArguments:
        "--job-language": scala
        "--class": GlueApp
        "--enable-glue-datacatalog": ""
        "--job-bookmark-option": "job-bookmark-enable"
        "--database_name": !Ref Database
        "--raw_table_name": raw_clicks # Hard-coded because CloudFormation-based Crawlers can't crawl an existing table
        "--table_name": !Ref ClicksTable
      ExecutionProperty:
        MaxConcurrentRuns: 1
      MaxRetries: 0

  #########
  # Batch #
  #########

  AdvertisingTable:
    Type: AWS::Glue::Table
    DependsOn:
      - Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref Database
      TableInput:
          Name: advertising
          Description: Online Advertising
          TableType: EXTERNAL_TABLE
          Parameters:
            classification: orc
          PartitionKeys:
            - Name: year
              Type: int
            - Name: month
              Type: int
          StorageDescriptor:
            Location: !Sub "s3://${BucketName}/data/lake/advertising/"
            StoredAsSubDirectories: true
            InputFormat: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            SerdeInfo:
              SerializationLibrary: org.apache.hadoop.hive.ql.io.orc.OrcSerde
            OutputFormat: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            Columns:
              - Name: ad_id
                Type: string
              - Name: impression_at
                Type: timestamp
              - Name: click_at
                Type: timestamp

  AdTrafficTable:
    Type: AWS::DynamoDB::Table
    Properties:
      AttributeDefinitions:
        - AttributeName: ad_id
          AttributeType: S
        - AttributeName: day
          AttributeType: S
      KeySchema:
        - AttributeName: ad_id
          KeyType: HASH
        - AttributeName: day
          KeyType: RANGE
      BillingMode: PAY_PER_REQUEST
      SSESpecification:
        SSEEnabled: true

  PipelineRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - !Sub "logs.${AWS::Region}.${AWS::URLSuffix}"
                - !Sub "datapipeline.${AWS::URLSuffix}"
                - !Sub "dynamodb.${AWS::URLSuffix}"
                - !Sub "ec2.${AWS::URLSuffix}"
                - !Sub "elasticmapreduce.${AWS::URLSuffix}"
                - !Sub "iam.${AWS::URLSuffix}"
                - !Sub "rds.${AWS::URLSuffix}"
                - !Sub "redshift.${AWS::URLSuffix}"
                - !Sub "s3.${AWS::URLSuffix}"
                - !Sub "sns.${AWS::URLSuffix}"
                - !Sub "sqs.${AWS::URLSuffix}"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSDataPipelineRole
      Policies:
        - PolicyName: SpectrumAccess
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - glue:*
                  - athena:*
                Effect: Allow
                Resource: "*"
              - Action:
                  - s3:Get*
                  - s3:List*
                Effect: Allow
                Resource:
                  - !Sub "${BucketARN}/data/raw/*"
                  - !Sub "${BucketARN}/data/lake/*"
                  - !Sub "${BucketARN}/data/temp/*"

  PipelineProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      InstanceProfileName: !Ref PipelineRole
      Roles:
        - !Ref PipelineRole

  PipelineResourceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - !Sub "logs.${AWS::Region}.${AWS::URLSuffix}"
                - !Sub "datapipeline.${AWS::URLSuffix}"
                - !Sub "dynamodb.${AWS::URLSuffix}"
                - !Sub "ec2.${AWS::URLSuffix}"
                - !Sub "elasticmapreduce.${AWS::URLSuffix}"
                - !Sub "iam.${AWS::URLSuffix}"
                - !Sub "rds.${AWS::URLSuffix}"
                - !Sub "redshift.${AWS::URLSuffix}"
                - !Sub "s3.${AWS::URLSuffix}"
                - !Sub "sns.${AWS::URLSuffix}"
                - !Sub "sqs.${AWS::URLSuffix}"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforDataPipelineRole
      Policies:
        - PolicyName: SpectrumAccess
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - glue:*
                  - athena:*
                Effect: Allow
                Resource: "*"
              - Action:
                  - s3:Get*
                  - s3:List*
                Effect: Allow
                Resource:
                  - !Sub "${BucketARN}/data/raw/*"
                  - !Sub "${BucketARN}/data/lake/*"
                  - !Sub "${BucketARN}/data/temp/*"

  PipelineResourceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      InstanceProfileName: !Ref PipelineResourceRole
      Roles:
        - !Ref PipelineResourceRole

  EMRPipeline:
    Type: AWS::DataPipeline::Pipeline
    DependsOn:
      - PipelineProfile
      - PipelineResourceProfile
    Properties:
      Name: EMR Pipeline
      Activate: false
      ParameterObjects:
        - Id: myRegion
          Attributes:
            - Key: type
              StringValue: String
            - Key: default
              StringValue: !Ref AWS::Region
      ParameterValues:
        - Id: myRegion
          StringValue: !Ref AWS::Region
      PipelineObjects:
        - Id: DailySchedule
          Name: Daily Schedule
          Fields:
            - Key: type
              StringValue: Schedule
            - Key: period
              StringValue: 1 days
            - Key: startDateTime
              StringValue: "2019-05-21T00:00:00"
            - Key: occurrences
              StringValue: 1
        - Id: Default
          Name: Default
          Fields:
            - Key: type
              StringValue: Default
            - Key: schedule
              RefValue: DailySchedule
            - Key: scheduleType
              StringValue: timeseries
            - Key: role
              StringValue: !Ref PipelineRole
            - Key: resourceRole
              StringValue: !Ref PipelineResourceRole
            - Key: pipelineLogUri
              StringValue: !Sub "s3://${BucketName}/logs/data-pipeline/"
            - Key: failureAndRerunMode
              StringValue: cascade
        - Id: HiveMetastoreClientFactoryClass
          Name: Hive Metastore Client Factory Class
          Fields:
            - Key: type
              StringValue: Property
            - Key: key
              StringValue: hive.metastore.client.factory.class
            - Key: value
              StringValue: com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory
        - Id: HiveMetastoreCatalog
          Name: Hive Metastore Catalog
          Fields:
            - Key: type
              StringValue: Property
            - Key: key
              StringValue: hive.metastore.glue.catalogid
            - Key: value
              StringValue: !Ref AWS::AccountId
        - Id: GlueCatalogAsHiveMetastore
          Name: Glue Catalog as Hive Metastore
          Fields:
            - Key: type
              StringValue: EmrConfiguration
            - Key: classification
              StringValue: hive-site
            - Key: property
              RefValue: HiveMetastoreClientFactoryClass
            - Key: property
              RefValue: HiveMetastoreCatalog
        - Id: GlueCatalogAsSparkMetastore
          Name: Glue Catalog as Spark Metastore
          Fields:
            - Key: type
              StringValue: EmrConfiguration
            - Key: classification
              StringValue: spark-hive-site
            - Key: property
              RefValue: HiveMetastoreClientFactoryClass
            - Key: property
              RefValue: HiveMetastoreCatalog
        - Id: EMRCluster
          Name: EMR Cluster
          Fields:
            - Key: type
              StringValue: EmrCluster
            - Key: keyPair
              StringValue: !Ref KeyPair
            - Key: masterInstanceType
              StringValue: m1.medium
            - Key: coreInstanceType
              StringValue: m1.medium
            - Key: coreInstanceCount
              StringValue: 10
            - Key: releaseLabel
              StringValue: emr-5.23.0
            - Key: applications
              StringValue: hive
            - Key: applications
              StringValue: pig
            - Key: applications
              StringValue: spark
            - Key: terminateAfter
              StringValue: 1 hours
            - Key: configuration
              RefValue: GlueCatalogAsHiveMetastore
            - Key: configuration
              RefValue: GlueCatalogAsSparkMetastore
        - Id: ExtractAdvertisingData
          Name: Extract Advertising Data
          Fields:
            - Key: type
              StringValue: HiveActivity
            - Key: scriptUri
              StringValue: !Sub "s3://${CodeBucket}/${CodePrefix}/scripts/hive/extract_advertising_data.hql"
            - !If
                - EMREnabled
                - Key: workerGroup
                  StringValue: !Sub "${AWS::StackName}-${AWS::Region}-emr-worker-group"
                - Key: runsOn
                  RefValue: EMRCluster
            - Key: stage
              StringValue: false
            - Key: scriptVariable
              StringValue: PARTITION_YEAR=#{year(@scheduledStartTime)}
            - Key: scriptVariable
              StringValue: PARTITION_MONTH=#{month(@scheduledStartTime)}
            - Key: scriptVariable
              StringValue: DATE_START=#{format(@scheduledStartTime, 'YYYY-MM-dd')}
            - Key: scriptVariable
              StringValue: DATE_END=#{format(@scheduledEndTime, 'YYYY-MM-dd')}
        - Id: AdTrafficTableExists
          Name: Ad Traffic Table Exists
          Fields:
            - Key: type
              StringValue: DynamoDBTableExists
            - Key: tableName
              StringValue: !Ref AdTrafficTable
        - Id: AdTrafficStagingTableFormat
          Name: Ad Traffic Staging Table Format
          Fields:
            - Key: type
              StringValue: DynamoDBExportDataFormat
            - Key: column
              StringValue: ad_id STRING
            - Key: column
              StringValue: day STRING
            - Key: column
              StringValue: impressions BIGINT
            - Key: column
              StringValue: clicks BIGINT
            - Key: column
              StringValue: clickthrough_rate DOUBLE
        - Id: AdTrafficStagingTable
          Name: Ad Traffic Staging Table
          Fields:
            - Key: type
              StringValue: S3DataNode
            - Key: directoryPath
              StringValue: !Sub "s3://${BucketName}/data/temp/advertising-info-staging-table/"
            - Key: dataFormat
              RefValue: AdTrafficStagingTableFormat
        - Id: DummyInputTableFormat
          Name: Dummy Input Table Format
          Fields:
            - Key: type
              StringValue: CSV
            - Key: column
              StringValue: ad_id STRING
            - Key: column
              StringValue: day STRING
            - Key: column
              StringValue: impressions BIGINT
            - Key: column
              StringValue: clicks BIGINT
            - Key: column
              StringValue: clickthrough_rate DOUBLE
        - Id: DummyInputTable
          Name: Dummy Input Table
          Fields:
            - Key: type
              StringValue: S3DataNode
            - Key: directoryPath
              StringValue: !Sub "s3://${BucketName}/data/temp/dummy/"
            - Key: dataFormat
              RefValue: DummyInputTableFormat
        - Id: AdTrafficTableFormat
          Name: Ad Traffic Table Format
          Fields:
            - Key: type
              StringValue: DynamoDBExportDataFormat
        - Id: AdTrafficTable
          Name: Ad Traffic Table
          Fields:
            - Key: type
              StringValue: DynamoDBDataNode
            - Key: tableName
              StringValue: !Ref AdTrafficTable
            - Key: dataFormat
              RefValue: AdTrafficTableFormat
            - Key: precondition
              RefValue: AdTrafficTableExists
        - Id: StageDataForAdTraffic
          Name: Stage Data for Ad Traffic
          Fields:
            - Key: type
              StringValue: HiveActivity
            - Key: dependsOn
              RefValue: ExtractAdvertisingData
            - Key: scriptUri
              StringValue: !Sub "s3://${CodeBucket}/${CodePrefix}/scripts/hive/stage_data_for_ad_traffic.hql"
            - !If
                - EMREnabled
                - Key: workerGroup
                  StringValue: !Sub "${AWS::StackName}-${AWS::Region}-emr-worker-group"
                - Key: runsOn
                  RefValue: EMRCluster
            - Key: input
              RefValue: DummyInputTable
            - Key: output
              RefValue: AdTrafficStagingTable
            - Key: scriptVariable
              StringValue: PARTITION_YEAR=#{year(@scheduledStartTime)}
            - Key: scriptVariable
              StringValue: PARTITION_MONTH=#{month(@scheduledStartTime)}
            - Key: scriptVariable
              StringValue: DATE_START=#{format(@scheduledStartTime, 'YYYY-MM-dd')}
            - Key: scriptVariable
              StringValue: DATE_END=#{format(@scheduledEndTime, 'YYYY-MM-dd')}
        - Id: LoadAdTrafficTable
          Name: Load Ad Traffic Into DynamoDB Table
          Fields:
            - Key: type
              StringValue: HiveCopyActivity
            - Key: dependsOn
              RefValue: StageDataForAdTraffic
            - Key: input
              RefValue: AdTrafficStagingTable
            - Key: output
              RefValue: AdTrafficTable
            - !If
                - EMREnabled
                - Key: workerGroup
                  StringValue: !Sub "${AWS::StackName}-${AWS::Region}-emr-worker-group"
                - Key: runsOn
                  RefValue: EMRCluster
        - Id: DummyOutputTableFormat
          Name: Dummy Output Table Format
          Fields:
            - Key: type
              StringValue: CSV
            - Key: column
              StringValue: rows INT
        - Id: DummyOutputTable
          Name: Dummy Output Table
          Fields:
            - Key: type
              StringValue: S3DataNode
            - Key: directoryPath
              StringValue: !Sub "s3://${BucketName}/data/temp/dummy/"
            - Key: dataFormat
              RefValue: DummyOutputTableFormat
        - Id: TruncateAdTrafficStagingTable
          Name: Truncate Ad Traffic Staging Table
          Fields:
            - Key: type
              StringValue: HiveActivity
            - Key: dependsOn
              RefValue: LoadAdTrafficTable
            - Key: hiveScript
              StringValue: TRUNCATE TABLE ${input1}
            - !If
                - EMREnabled
                - Key: workerGroup
                  StringValue: !Sub "${AWS::StackName}-${AWS::Region}-emr-worker-group"
                - Key: runsOn
                  RefValue: EMRCluster
            - Key: input
              RefValue: AdTrafficStagingTable
            - Key: output
              RefValue: DummyOutputTable

  RedshiftPipeline:
    Type: AWS::DataPipeline::Pipeline
    Condition: RedshiftEnabled
    DependsOn:
      - PipelineProfile
      - PipelineResourceProfile
    Properties:
      Name: Redshift Pipeline
      Activate: false
      ParameterObjects:
        - Id: myRegion
          Attributes:
            - Key: type
              StringValue: String
            - Key: default
              StringValue: !Ref AWS::Region
      ParameterValues:
        - Id: myRegion
          StringValue: !Ref AWS::Region
      PipelineObjects:
        - Id: DailySchedule
          Name: Daily Schedule
          Fields:
            - Key: type
              StringValue: Schedule
            - Key: period
              StringValue: 1 days
            - Key: startDateTime
              StringValue: "2019-05-21T00:00:00"
            - Key: occurrences
              StringValue: 1
        - Id: Default
          Name: Default
          Fields:
            - Key: type
              StringValue: Default
            - Key: schedule
              RefValue: DailySchedule
            - Key: scheduleType
              StringValue: timeseries
            - Key: role
              StringValue: !Ref PipelineRole
            - Key: resourceRole
              StringValue: !Ref PipelineResourceRole
            - Key: pipelineLogUri
              StringValue: !Sub "s3://${BucketName}/logs/data-pipeline/"
            - Key: failureAndRerunMode
              StringValue: cascade
        - Id: EC2Instance
          Name: EC2 Instance
          Fields:
            - Key: type
              StringValue: Ec2Resource
            - Key: actionOnTaskFailure
              StringValue: terminate
            - Key: actionOnResourceFailure
              StringValue: retryAll
            - Key: maximumRetries
              StringValue: 1
            - Key: instanceType
              StringValue: m1.medium
            - Key: subnetId
              StringValue: !Ref Subnet
            - Key: securityGroupIds
              StringValue: !Ref SecurityGroup
            - Key: associatePublicIpAddress
              StringValue: true
            - Key: keyPair
              StringValue: !Ref KeyPair
            - Key: terminateAfter
              StringValue: 1 hours
        - Id: HiveMetastoreClientFactoryClass
          Name: Hive Metastore Client Factory Class
          Fields:
            - Key: type
              StringValue: Property
            - Key: key
              StringValue: hive.metastore.client.factory.class
            - Key: value
              StringValue: com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory
        - Id: HiveMetastoreCatalog
          Name: Hive Metastore Catalog
          Fields:
            - Key: type
              StringValue: Property
            - Key: key
              StringValue: hive.metastore.glue.catalogid
            - Key: value
              StringValue: !Ref AWS::AccountId
        - Id: GlueCatalogAsHiveMetastore
          Name: Glue Catalog as Hive Metastore
          Fields:
            - Key: type
              StringValue: EmrConfiguration
            - Key: classification
              StringValue: hive-site
            - Key: property
              RefValue: HiveMetastoreClientFactoryClass
            - Key: property
              RefValue: HiveMetastoreCatalog
        - Id: GlueCatalogAsSparkMetastore
          Name: Glue Catalog as Spark Metastore
          Fields:
            - Key: type
              StringValue: EmrConfiguration
            - Key: classification
              StringValue: spark-hive-site
            - Key: property
              RefValue: HiveMetastoreClientFactoryClass
            - Key: property
              RefValue: HiveMetastoreCatalog
        - Id: EMRCluster
          Name: EMR Cluster
          Fields:
            - Key: type
              StringValue: EmrCluster
            - Key: keyPair
              StringValue: !Ref KeyPair
            - Key: masterInstanceType
              StringValue: m1.medium
            - Key: coreInstanceType
              StringValue: m1.medium
            - Key: coreInstanceCount
              StringValue: 10
            - Key: releaseLabel
              StringValue: emr-5.23.0
            - Key: applications
              StringValue: hive
            - Key: applications
              StringValue: pig
            - Key: applications
              StringValue: spark
            - Key: terminateAfter
              StringValue: 1 hours
            - Key: configuration
              RefValue: GlueCatalogAsHiveMetastore
            - Key: configuration
              RefValue: GlueCatalogAsSparkMetastore
        - Id: RedshiftCluster
          Name: Redshift Cluster
          Fields:
            - Key: type
              StringValue: RedshiftDatabase
            - Key: clusterId
              StringValue: !Ref RedshiftClusterId
            - Key: username
              StringValue: !Ref RedshiftUsername
            - Key: "*password"
              StringValue: !Ref RedshiftPassword
            - Key: databaseName
              StringValue: !Ref RedshiftDatabase
            - Key: region
              StringValue: !Ref AWS::Region
        - Id: RedshiftAdvertisingAdTable
          Name: Redshift Advertising by Ad Table
          Fields:
            - Key: type
              StringValue: RedshiftDataNode
            - Key: database
              RefValue: RedshiftCluster
            - Key: tableName
              StringValue: advertising
            - Key: createTableSql
              StringValue: |
                CREATE TABLE advertising (
                  ad_id         VARCHAR(64) NOT NULL,
                  impression_at TIMESTAMP   NOT NULL,
                  click_at      TIMESTAMP
                )
                DISTSTYLE KEY
                DISTKEY (ad_id)
                ;
        - Id: DummyInputTableFormat
          Name: Dummy Input Table Format
          Fields:
            - Key: type
              StringValue: CSV
            - Key: column
              StringValue: ad_id STRING
            - Key: column
              StringValue: impression_at TIMESTAMP
            - Key: column
              StringValue: click_at TIMESTAMP
        - Id: DummyInputTable
          Name: Dummy Input Table
          Fields:
            - Key: type
              StringValue: S3DataNode
            - Key: directoryPath
              StringValue: !Sub "s3://${BucketName}/data/raw/ad/impressions/"
            - Key: dataFormat
              RefValue: DummyInputTableFormat
        - Id: RedshiftStagingTableFormat
          Name: Redshift Staging Table Format
          Fields:
            - Key: type
              StringValue: CSV
            - Key: column
              StringValue: ad_id STRING
            - Key: column
              StringValue: impression_at TIMESTAMP
            - Key: column
              StringValue: click_at TIMESTAMP
        - Id: RedshiftStagingTable
          Name: Redshift Staging Table
          Fields:
            - Key: type
              StringValue: S3DataNode
            - Key: directoryPath
              StringValue: !Sub "s3://${BucketName}/data/temp/redshift-staging-table/"
            - Key: dataFormat
              RefValue: RedshiftStagingTableFormat
        - Id: StageDataForRedshift
          Name: Stage Data for Redshift
          Fields:
            - Key: type
              StringValue: HiveActivity
            - Key: scriptUri
              StringValue: !Sub "s3://${CodeBucket}/${CodePrefix}/scripts/hive/stage_data_for_redshift.hql"
            - !If
                - EMREnabled
                - Key: workerGroup
                  StringValue: !Sub "${AWS::StackName}-${AWS::Region}-emr-worker-group"
                - Key: runsOn
                  RefValue: EMRCluster
            - Key: input
              RefValue: DummyInputTable
            - Key: output
              RefValue: RedshiftStagingTable
            - Key: scriptVariable
              StringValue: PARTITION_YEAR=#{year(@scheduledStartTime)}
            - Key: scriptVariable
              StringValue: PARTITION_MONTH=#{month(@scheduledStartTime)}
            - Key: scriptVariable
              StringValue: DATE_START=#{format(@scheduledStartTime, 'YYYY-MM-dd')}
            - Key: scriptVariable
              StringValue: DATE_END=#{format(@scheduledEndTime, 'YYYY-MM-dd')}
        - Id: LoadAdvertisingIntoRedshift
          Name: Load Advertising Into Redshift
          Fields:
            - Key: type
              StringValue: RedshiftCopyActivity
            - Key: dependsOn
              RefValue: StageDataForRedshift
            - Key: input
              RefValue: RedshiftStagingTable
            - Key: output
              RefValue: RedshiftAdvertisingAdTable
            - Key: insertMode
              StringValue: OVERWRITE_EXISTING
            - !If
              - EC2InstanceEnabled
              - Key: workerGroup
                StringValue: !Sub "${AWS::StackName}-${AWS::Region}-instance-worker-group"
              - Key: runsOn
                RefValue: EC2Instance
        - Id: DummyOutputTableFormat
          Name: Dummy Output Table Format
          Fields:
            - Key: type
              StringValue: CSV
            - Key: column
              StringValue: rows INT
        - Id: DummyOutputTable
          Name: Dummy Output Table
          Fields:
            - Key: type
              StringValue: S3DataNode
            - Key: directoryPath
              StringValue: !Sub "s3://${BucketName}/data/temp/dummy/"
            - Key: dataFormat
              RefValue: DummyOutputTableFormat
        - Id: TruncateRedshiftStagingTable
          Name: Truncate Redshift Staging Table
          Fields:
            - Key: type
              StringValue: HiveActivity
            - Key: dependsOn
              RefValue: LoadAdvertisingAdIntoRedshift
            - Key: dependsOn
              RefValue: LoadAdvertisingUserIntoRedshift
            - Key: hiveScript
              StringValue: TRUNCATE TABLE ${input1}
            - !If
                - EMREnabled
                - Key: workerGroup
                  StringValue: !Sub "${AWS::StackName}-${AWS::Region}-emr-worker-group"
                - Key: runsOn
                  RefValue: EMRCluster
            - Key: input
              RefValue: RedshiftStagingTable
            - Key: output
              RefValue: DummyOutputTable
        - Id: UseSpectrumDataLake
          Name: Use Spectrum Data Lake
          Fields:
            - Key: type
              StringValue: SqlActivity
            - !If
              - EC2InstanceEnabled
              - Key: workerGroup
                StringValue: !Sub "${AWS::StackName}-${AWS::Region}-instance-worker-group"
              - Key: runsOn
                RefValue: EC2Instance
            - Key: database
              RefValue: RedshiftCluster
            - Key: script
              StringValue: !Sub |
                CREATE EXTERNAL SCHEMA IF NOT EXISTS advertising
                FROM DATA CATALOG
                DATABASE '${Database}'
                IAM_ROLE '${PipelineRole.Arn}'
                CREATE EXTERNAL DATABASE
                IF NOT EXISTS;
        - Id: SpectrumLoadAdvertising
          Name: Spectrum Load Advertising
          Fields:
            - Key: type
              StringValue: SqlActivity
            - Key: dependsOn
              RefValue: UseSpectrumDataLake
            - Key: database
              RefValue: RedshiftCluster
            - !If
              - EC2InstanceEnabled
              - Key: workerGroup
                StringValue: !Sub "${AWS::StackName}-${AWS::Region}-instance-worker-group"
              - Key: runsOn
                RefValue: EC2Instance
            - Key: scriptUri
              StringValue: !Sub "s3://${CodeBucket}/${CodePrefix}/scripts/redshift/load_advertising_from_spectrum.sql"
            - Key: scriptArgument
              StringValue: "#{year(@scheduledStartTime)}"
            - Key: scriptArgument
              StringValue: "#{month(@scheduledStartTime)}"
            - Key: scriptArgument
              StringValue: "#{format(@scheduledStartTime, 'YYYY-MM-dd')}"
            - Key: scriptArgument
              StringValue: "#{format(@scheduledEndTime, 'YYYY-MM-dd')}"

  #######
  # API #
  #######

  APILambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - !Sub "lambda.${AWS::URLSuffix}"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/AWSXrayWriteOnlyAccess
      Policies:
        - PolicyName: Execution
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - firehose:PutRecordBatch
                Effect: Allow
                Resource:
                  - !GetAtt ImpressionsDeliveryStream.Arn
                  - !GetAtt ClicksDeliveryStream.Arn
              - Action:
                  - dynamodb:Query
                Effect: Allow
                Resource:
                  - !GetAtt AdTrafficTable.Arn

  CaptureImpressionsLambda:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: lambdas.zip
      Handler: bin/capture-impressions
      Role: !GetAtt APILambdaRole.Arn

  CaptureClicksLambda:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: lambdas.zip
      Handler: bin/capture-clicks
      Role: !GetAtt APILambdaRole.Arn

  QueryAdTrafficTableLambda:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: lambdas.zip
      Handler: bin/get-ad-traffic
      Role: !GetAtt APILambdaRole.Arn

  APIRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - !Sub "apigateway.${AWS::URLSuffix}"
      Policies:
        - PolicyName: Execution
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - lambda:InvokeFunction
                Effect: Allow
                Resource:
                  - !GetAtt CaptureImpressionsLambda.Arn
                  - !GetAtt CaptureClicksLambda.Arn
                  - !GetAtt QueryAdTrafficTableLambda.Arn

  API:
    Type: AWS::Serverless::Api
    Properties:
      StageName: !Ref Stage
      MethodSettings:
        - HttpMethod: "*"
          ResourcePath: "/*"
          # LoggingLevel: "ERROR"
          MetricsEnabled: true
      TracingEnabled: true
      DefinitionBody:
        'Fn::Transform':
          Name: 'AWS::Include'
          Parameters:
            Location: api.yaml
